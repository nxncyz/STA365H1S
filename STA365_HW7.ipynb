{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed893bc",
   "metadata": {},
   "source": [
    "**Ziyi Zhang 1005282720**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0bf84",
   "metadata": {},
   "source": [
    "# Homework 8: (a) Posterior Predictive Distributions<br> and (b) Missing Data Imputation\n",
    "\n",
    "### 1. Describe how the posterior predictive distribution is created for mixture models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eedad75",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02ef1a",
   "metadata": {},
   "source": [
    "The posterior predictive distribution for mixture models is a way to understand what new data might look like, based on the model fitted to existing data. In the context of Bayesian mixture models, the posterior predictive distribution takes in account the uncertainty in both the mixture components (like means, variances of different groups within the data) and the mixing proportions (the weights for each components).\n",
    "\n",
    "Steps of how the posterior predictive distribution is created for mixture models:\n",
    "\n",
    "1. Fit the Mixture model: THe mixture model is fitted to the data, typically using a Bayesian framework. This involves specifyig the mixture components (often Gaussian distributions for continous data), the mixing proportions (how much each component contirbutes to the total model), and prior distribution for all unknown parameters\n",
    "\n",
    "2. Sample from the Posterior: After fitting the model, can obtain samples from the posterior distribution of the paramteres using MCMC or variational inference. These samples represent the uncertainty in the parameter estimates given the observed data\n",
    "\n",
    "3. Generate Predictions: To create the posterior predicctive distribution, you generate new data points by randomly selecting from the mixture components for each sample fo parameters drawn from the posterior. The choice of mixture component can be based on the estimated mixing proportions\n",
    "\n",
    "4. Aggregate Predictions: The generated predictions across all posterior samples form the posterior predictivge distribution. This distribution represents what new data might look like, given the model and the observed data used to fit it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6386d44",
   "metadata": {},
   "source": [
    "### 2. Describe how the posterior predictive distribution is created in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6558332",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea71f4d2",
   "metadata": {},
   "source": [
    "The posterior predictive distribution represents the distribution of possible unobserved outcomes based on the model and the data that have been observed. It essentially tells us what we might expect in future data points given our current observations and model. The posterior predictive distribution is obtained by averaging over all the uncertainties in a model, both from the parameter estimation and from th einherent randomness described by the likelihood. It provides a compelte predicive distribution for future observations, including inherent randomness and uncertianty in the model parameters.  \n",
    "\n",
    "Here's a general process of how it's created:\n",
    "\n",
    "1. Model Specification: Start by specifying a probabilistic model for the observed data. This involves choosing a likelihood function $p(y|\\theta)$ that describes how the data $y$ is generated from a set of parameters $\\theta$\n",
    "\n",
    "2. Prior Distribution: Once a prior distribution $p(\\theta)$ for the parameters that encapsulates the uncertainty about their values before observing any data\n",
    "\n",
    "3. Posterior Distribution: After observing the data, update the beliefs about the parameters using Bayes' theorem to get the psoterior distribution: $p(\\theta|y) = \\frac{p(y|\\theta)p(\\theta)}{p(y)}$ where $p(y)$ is the evidence or marginal likelihood, often computed by integrating the likelihood over all possible parameter values weighted by their prior probability\n",
    "\n",
    "4. Sampling from the Posterior: Once we have the posterior distribution, we can draw samples from it. These samples represent different plausible values of the parameter $\\theta$ after taking the data into account\n",
    "\n",
    "5. Generating Predictive Outcomes: For each set of sampled parameters, generate predicted outcomes $y^*$ from the likelihood function $p(y^*|\\theta)$. This step is repeated for many samples from the posterior to create a distribution of possible future outcomes \n",
    "\n",
    "6. Creating the Posterior Predictive Distirbution: The collection of all these predicted outcomes forms the posterior predictive distributioin $p(y^*|y)$. It's essentially a compoun dporbability distirbution, obtained by averaging over all the uncertainties in the model paramters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b998c7c",
   "metadata": {},
   "source": [
    "### 3. Have glance through [this](https://www.pymc.io/projects/examples/en/latest/case_studies/Missing_Data_Imputation.html) and then describe how, if you were doing a regression of $y$ on $X$ but $X$ had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in $X$\n",
    "\n",
    "- **Hint: latent variables $v$ indicating the subpopulation are competely missing values that we simply treat as paramters to be inferred though posterior analysis... the same sort of thing can be done with missing values in data that need to be imputed... we should just be careful about the MCAR assumption...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555a503a",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d656bee2",
   "metadata": {},
   "source": [
    "To perform a Bayesian analysis with missing data in $X$, we can treat the missing values as unknown parameters in the model. This approach involves specifying a prior distribution for the missing data, which reflects our beliefs above their possible values before observing the data. Then use Bayesian inference to update this prior with information from th eobserved data. The imputation process involves sampling from the posterior distribution of the missing values given the observed data, which can be accomplished using a probabilistic programming tools like PyMC. This method allows us to integrate out the missing data uncertainty and incorporate it into the paramter estimates and predictions. Which typically results in a more robust analysis compared to discarding incomplete cases. Also being cautious abot the MCAR assumption is crucial in any imputation method. MCAR means that the likelihood of data being missing is independent of both obsesrved and unobserved data. When data are not MCAR, the missingness itself can contain information, and not accounting for the mechanism by which data are missing and can lead to biased estimates. Bayesian methods can incorporate models for the missing data mechanism, but the validity of the results can still depend heavily on the assumptions made about the reasons for the missing data. Important to consider whether these assumptions are justified given the context of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c642f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
